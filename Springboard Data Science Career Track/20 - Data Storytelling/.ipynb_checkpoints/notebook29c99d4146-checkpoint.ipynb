{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc5c83ea-f2dc-bd22-7baa-525bcef8ff97"
   },
   "outputs": [],
   "source": [
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pointbiserialr, spearmanr\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# for logit regression. \n",
    "# statsmodel is chosen because it outputs descriptive stats for the model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# for SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for random forest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79084389-2c25-9a10-ca14-c29990238cf8"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/adult.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d62e85f0-4d72-77af-b5e6-fa0010d7f9fc"
   },
   "source": [
    "# **Deal with missing values**\n",
    "\n",
    "Here the dataset is not using the default nan string for missing values, instead \"?\" is used\n",
    "\n",
    "Hence we check occurrences of \"?\" in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d0f1c96-a3b2-d1f0-d3dd-cf7ee2f0853f"
   },
   "outputs": [],
   "source": [
    "col_names = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_names:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print (c)\n",
    "        print (num_non)\n",
    "        print (\"{0:.2f}%\".format(float(num_non) / num_data * 100))\n",
    "        print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5c1e1ce9-0546-3b9a-e77c-e3845ed2820f"
   },
   "source": [
    "All three columns are categorical columns. It takes much more extra work to estimate values for the missing entries. \n",
    "\n",
    "Considering the relative low portion of missing data, we discard rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f66da66-b678-d959-ad9b-f0fa6f14ad38"
   },
   "outputs": [],
   "source": [
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "860f4477-a185-3cc5-4acf-1e4f38bf38ec"
   },
   "source": [
    "After cleaning the data, we get 45,222 rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7bcd268c-f5e0-fe26-8c90-d51a549bde70"
   },
   "source": [
    "# **Explore the cleaned data**\n",
    "\n",
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c803474-1713-995d-eb7d-15df52e45760"
   },
   "outputs": [],
   "source": [
    "# descriptive stats for numerical fields\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c220f292-c51a-6373-70cc-e72394c7bca6"
   },
   "source": [
    "fnlwgt appears to be a highly dispersive. In fact, it is a weight on the Current Population Survey (CPS) files. We will not incorporate fnlwgt in set of predictor variables     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c08925ef-db0b-1d0c-c1b9-479e587b7f68"
   },
   "outputs": [],
   "source": [
    "# frequency for categorical fields \n",
    "category_col =['workclass', 'race', 'education','marital-status', 'occupation',\n",
    "               'relationship', 'gender', 'native-country', 'income'] \n",
    "for c in category_col:\n",
    "    print (c)\n",
    "    print (data[c].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd3f48de-76ca-63f4-28de-35115d78d38c"
   },
   "source": [
    "The field marital-status are too detailed. It can be re-categorizing into more general terms. Discretisation will be applied on this field later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "05cef31a-aa2f-71f3-5351-42ad2e672cb4"
   },
   "source": [
    "Now how about the target value?\n",
    "\n",
    "We are faced with a classification problem on two classes. \n",
    "\n",
    "Are the two classes balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ad08556b-5549-9f3a-138c-6c687353c86e"
   },
   "outputs": [],
   "source": [
    "data[\"income\"].value_counts()[0] / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "86d599fd-876d-f868-88ae-7c9782fe8aca"
   },
   "outputs": [],
   "source": [
    "data[\"income\"].value_counts()[1] / data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "66e917c7-b4cf-a3c0-69d7-7ffd358d473b"
   },
   "source": [
    "The two classes are imbalanced.\n",
    "\n",
    "Stratified sampling will be adopted in dividing train and test set to preserve the ratio between two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a70dfd1-50ea-7543-02e7-a5a8ded76681"
   },
   "source": [
    "# **Deal with categorical columns** \n",
    "\n",
    "To fit the data into prediction model, we need convert categorical values to numerical ones. \n",
    "\n",
    "Before that, we will evaluate if any transformation on categorical columns are necessary. \n",
    "\n",
    "Discretisation is a common way to make categorical data more tidy and meaningful.\n",
    "\n",
    "Here we apply discretisation on column marital_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "723438f4-7b54-f4c4-9cd6-c16e4596138c"
   },
   "outputs": [],
   "source": [
    "data.replace(['Divorced', 'Married-AF-spouse', \n",
    "              'Married-civ-spouse', 'Married-spouse-absent', \n",
    "              'Never-married','Separated','Widowed'],\n",
    "             ['not married','married','married','married',\n",
    "              'not married','not married','not married'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "97f47143-569d-38fc-852b-57f03e1c3072"
   },
   "source": [
    "Now we can convert categorical columns to numerical representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c62546ab-ef9a-a12f-3d7e-8793a06bd818"
   },
   "outputs": [],
   "source": [
    "for col in category_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True) \n",
    "    data[col] = c\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10d5a842-1448-3996-82e3-d6f0abc60d6e"
   },
   "source": [
    "# Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4cf4988e-606b-87de-b38b-eee162058324"
   },
   "outputs": [],
   "source": [
    "col_names = data.columns\n",
    "\n",
    "param=[]\n",
    "correlation=[]\n",
    "abs_corr=[]\n",
    "\n",
    "for c in col_names:\n",
    "    #Check if binary or continuous\n",
    "    if c != \"income\":\n",
    "        if len(data[c].unique()) <= 2:\n",
    "            corr = spearmanr(data['income'],data[c])[0]\n",
    "        else:\n",
    "            corr = pointbiserialr(data['income'],data[c])[0]\n",
    "        param.append(c)\n",
    "        correlation.append(corr)\n",
    "        abs_corr.append(abs(corr))\n",
    "\n",
    "#Create dataframe for visualization\n",
    "param_df=pd.DataFrame({'correlation':correlation,'parameter':param, 'abs_corr':abs_corr})\n",
    "\n",
    "#Sort by absolute correlation\n",
    "param_df=param_df.sort_values(by=['abs_corr'], ascending=False)\n",
    "\n",
    "#Set parameter name as index\n",
    "param_df=param_df.set_index('parameter')\n",
    "\n",
    "param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40b1ec3a-85b1-b930-bca6-f38a0214c74e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ab5966f8-5db1-025a-ddca-e86a93c0449b"
   },
   "outputs": [],
   "source": [
    "scoresCV = []\n",
    "scores = []\n",
    "\n",
    "for i in range(1,len(param_df)):\n",
    "    new_df=data[param_df.index[0:i+1].values]\n",
    "    X = new_df.ix[:,1::]\n",
    "    y = new_df.ix[:,0]\n",
    "    clf = DecisionTreeClassifier()\n",
    "    scoreCV = cross_val_score(clf, X, y, cv= 10)\n",
    "    scores.append(np.mean(scoreCV))\n",
    "    \n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(1,len(scores)+1),scores, '.-')\n",
    "plt.axis(\"tight\")\n",
    "plt.title('Feature Selection', fontsize=14)\n",
    "plt.xlabel('# Features', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "29596dfe-e1c2-9695-59c3-92ef4049285c"
   },
   "outputs": [],
   "source": [
    "best_features=param_df.index[0:4].values\n",
    "print('Best features:\\t',best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62119fe3-006d-1dd6-443b-f06ee2d31c2d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43f8a885-9228-1b17-2350-e03da12135ab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50a0d636-b7ec-8290-6283-b61a1115c567"
   },
   "source": [
    "# **Prediction**\n",
    "\n",
    "Let's test out 3 common machine learning algorithms.\n",
    "\n",
    " 1. Logistic regression\n",
    " 2. Support Vector Machine\n",
    " 3. Random Forest\n",
    "\n",
    "The test design follows these ideas:\n",
    "\n",
    " - The dataset will be divided into 3 sets: train, test and cross validation\n",
    " - Stratified sampling is used as our target value is unbalanced. In each set, we maintain the ratio of high income entries over low income entries as 24.78/75.22, the same ratio as it is in the full dataset without missing values.\n",
    " - Accuracy (ACC) and area-under-curve (AUC) are used as an evaluation of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78b3b4c9-fc0e-800b-4cfd-a6501c40e18d"
   },
   "source": [
    "## logistic regression\n",
    "\n",
    "\n",
    "\n",
    "Here I put up a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7188bf85-08ff-cde1-ad95-ef0d728eb8b2"
   },
   "outputs": [],
   "source": [
    "predictors = ['age','workclass','education','educational-num',\n",
    "              'marital-status', 'occupation','relationship','race','gender',\n",
    "              'capital-gain','capital-loss','hours-per-week', 'native-country']\n",
    "\n",
    "#predictors = ['marital-status', 'educational-num', 'relationship', 'age']\n",
    "\n",
    "high_income = data[data['income'] == 1]\n",
    "low_income = data[data['income'] == 0]\n",
    "\n",
    "# stratified sampling\n",
    "#80% to train set\n",
    "train = pd.concat([high_income.sample(frac=0.8, random_state=1),\n",
    "                   low_income.sample(frac=0.8, random_state=1)]) \n",
    "y_train = train[\"income\"]\n",
    "X_train = train[predictors]\n",
    "\n",
    "#10% to test set\n",
    "test = pd.concat([high_income.sample(frac=0.1, random_state=1), \n",
    "                  low_income.sample(frac=0.1, random_state=1)])\n",
    "y_test = test[\"income\"]\n",
    "X_test = test[predictors]\n",
    "\n",
    "#10% to CV set\n",
    "cross = pd.concat([high_income.sample(frac=0.1, random_state=2), \n",
    "                   low_income.sample(frac=0.1, random_state=2)])\n",
    "y_cross = cross[\"income\"]\n",
    "X_cross = cross[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "281e03bf-01f1-c3d7-6d27-cecf0700f9cf"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "print(\"train set result\\n\")\n",
    "logit_train = sm.Logit(y_train, X_train) \n",
    "result_train = logit_train.fit()\n",
    "\n",
    "y_train_pred = result_train.predict(X_train) \n",
    "y_train_pred = (y_train_pred > 0.5).astype(int) \n",
    "acc = accuracy_score(y_train, y_train_pred) \n",
    "print(\"ACC=%f\" % (acc))\n",
    "auc = roc_auc_score(y_train, y_train_pred) \n",
    "print(\"AUC=%f\" % (auc))\n",
    "\n",
    "print(\"\\n CV set result\\n\")\n",
    "y_cross_pred = result_train.predict(X_cross) \n",
    "y_cross_pred = (y_cross_pred > 0.5).astype(int)\n",
    "acc = accuracy_score(y_cross, y_cross_pred) \n",
    "print(\"ACC=%f\" % (acc))\n",
    "auc = roc_auc_score(y_cross, y_cross_pred) \n",
    "print (\"AUC=%f\" % (auc))\n",
    "\n",
    "print(\"\\n test set result\\n\")\n",
    "y_test_pred = result_train.predict(X_test) \n",
    "y_test_pred = (y_test_pred > 0.5).astype(int) \n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "print(\"ACC=%f\" % (acc))\n",
    "auc = roc_auc_score(y_test, y_test_pred)\n",
    "print(\"AUC=%f\" % (auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0c2a26e9-bb74-99e6-9a9e-b44595becbc7"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f0d0940-f6b3-8f80-6358-f17557402f41"
   },
   "outputs": [],
   "source": [
    "predictors = ['age','workclass','education','educational-num',\n",
    "              'marital-status', 'occupation','relationship','race','gender',\n",
    "              'capital-gain','capital-loss','hours-per-week', 'native-country']\n",
    "\n",
    "predictors = ['marital-status', 'educational-num', 'relationship', 'age']\n",
    "\n",
    "pred_data = data[predictors] #X\n",
    "target = data[\"income\"] #y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "71c61f38-74af-3fb8-8b18-7fafc36df868"
   },
   "outputs": [],
   "source": [
    "algorithms = [ \n",
    "    #linear kernel\n",
    "    [Pipeline([('scaler',StandardScaler()), \n",
    "               ('svc',LinearSVC(random_state=1))]), predictors],\n",
    "    #rbf kernel\n",
    "    [Pipeline([('scaler',StandardScaler()),\n",
    "               ('svc',SVC(kernel=\"rbf\", random_state=1))]), predictors],\n",
    "    #polynomial kernel\n",
    "    [Pipeline([('scaler',StandardScaler()),\n",
    "               ('svc',SVC(kernel='poly', random_state=1))]), predictors],\n",
    "    #sigmoidf kernel\n",
    "    [Pipeline([('scaler',StandardScaler()),\n",
    "               ('svc',SVC(kernel='sigmoid', random_state=1))]), predictors]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9564916-becb-dd4d-3489-a71d1b3d486b"
   },
   "outputs": [],
   "source": [
    "alg_acc = {}\n",
    "alg_auc = {}\n",
    "for alg, predictors in algorithms:\n",
    "    alg_acc[alg] = 0\n",
    "    alg_auc[alg] = 0\n",
    "i=0\n",
    "\n",
    "pred_data = data[predictors] #X\n",
    "target = data[\"income\"] #y\n",
    "\n",
    "#stratified sampling\n",
    "#random_state=1: we get the same splits every time we run this\n",
    "# sss = StratifiedShuffleSplit(target, 10, test_size=0.1, random_state=1) \n",
    "sss = StratifiedShuffleSplit(target, 1, test_size=0.1, random_state=1) \n",
    "for train_index, test_index in sss:\n",
    "    i += 1\n",
    "    train_data = data.iloc[train_index]\n",
    "    test_data = data.iloc[test_index]\n",
    "    train_data = pd.concat([train_data,\n",
    "                            train_data[train_data[\"income\"]==1],\n",
    "                            train_data[train_data[\"income\"]==1]]) \n",
    "    X_train, X_test = train_data[predictors], test_data[predictors] \n",
    "    y_train, y_test = train_data[\"income\"], test_data[\"income\"]\n",
    "    \n",
    "    #Make predictions for each algorithm on each fold for alg, predictors in algorithms:\n",
    "    for alg, predictors in algorithms:\n",
    "        alg.fit(X_train, y_train)\n",
    "        y_pred = alg.predict(X_test)\n",
    "        acc_score = accuracy_score(y_test, y_pred) \n",
    "        print(acc_score)\n",
    "        alg_acc[alg] += acc_score\n",
    "        auc_score = roc_auc_score(y_test, y_pred) \n",
    "        print(auc_score)\n",
    "        alg_auc[alg] += auc_score\n",
    "\n",
    "for alg, predictors in algorithms:\n",
    "    alg_acc[alg] /= 1\n",
    "    alg_auc[alg] /= 1\n",
    "    print (\"## %s ACC=%f\" % (alg, alg_acc[alg]))\n",
    "    print (\"## %s AUC=%f\" % (alg, alg_auc[alg]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35545dde-b78f-4f3f-16c2-f31d4f4bd9d0"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "57b1b560-e749-4c15-a99d-b898dc974365"
   },
   "outputs": [],
   "source": [
    "#Bagging\n",
    "tree_count = 10 \n",
    "bag_proportion = 0.6 \n",
    "predictions = []\n",
    "\n",
    "sss = StratifiedShuffleSplit(target, 1, test_size=0.1, random_state=1) \n",
    "for train_index, test_index in sss:\n",
    "    train_data = data.iloc[train_index] \n",
    "    test_data = data.iloc[test_index]\n",
    "    \n",
    "    for i in range(tree_count):\n",
    "        bag = train_data.sample(frac=bag_proportion, replace = True, random_state=i)\n",
    "        X_train, X_test = bag[predictors], test_data[predictors]\n",
    "        y_train, y_test = bag[\"income\"], test_data[\"income\"]\n",
    "        clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=75) \n",
    "        clf.fit(X_train, y_train) \n",
    "        predictions.append(clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "combined = np.sum(predictions, axis=0)/10 \n",
    "rounded= np.round(combined)\n",
    "\n",
    "print(accuracy_score(rounded, y_test)) \n",
    "print(roc_auc_score(rounded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ae71a53e-4197-bac9-535d-16a8cf3ea5b9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
